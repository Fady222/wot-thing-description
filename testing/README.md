# Implementation Results
This directory containts the WoT Thing Description Implementation report
and the raw data for it.
The report itself is in [plan.html](plan.html).
Do not edit this file,
it is autogenerated by executing 'npm run assertions' from the main directory.

## Report Template and Implementation Descriptions
The HTML template for the Implementation Report is in [template.html](template.html).
This template also includes the descriptions of each implementation.
The id's declared in these descriptions should be used elsewhere,
for example, in data files referring to those implementations.

To add a new description, use the template in [impl-template.html](impl-template.html).

## Test Specifications
The procedure for testing each normative assertion should be given in
[testspec.html](testspec.html).  These will be included as an 
appendix in the report.   Test specifications can be given both for
assertions in the specification and extra assertions in 
[extra-asserts.html](extra-asserts.html).

## Extra Assertions
Assertions used for testing but not (yet) included in the specification
may be listed in this file.
The intention is that these assertions should
eventually, and before final release, be inserted into the final specification.

## At-Risk Assertions
Assertions related to features that are at risk of being deleted from the final
CR should be listed in the [atrisk.csv](atrisk.csv) file.
The assertion text for these will be colored in magenta in the report table.

## Categories
Assertions can be assigned to a category in
[categories.csv](categories.csv) (Eventually the table will group categories
into sections; for now it is just an extra column).

## Result Data
Each implementation should record
which features they have implemented and tested under the `results` directory.
All data will be read and merged into the report.
Mark each implemented feature with a status of either "pass" (if it satisfies the specification)
or "fail" (if it does not).  
If you have not implemented a feature, list its status as "not-impl".
Features not listed will not be included in the sums (TODO: maybe they should
instead be treated as non-implemented).  
Any other status will be ignored (e.g. "null" as used in the template).
If you have tested a feature in multiple
implementations check in one file per implementation, using as a filename
the id given in the template for the implementations' description.
Use a convention
like corporation-implementation.csv for the filename.
The filename should also be used as an id in the template
to describe each implementation.

The template.csv file lists all features but with a "null" status.
Do not edit this; it is autogenerated.
It is provided so
you can use it as a reference and as a basis for your own data files.

Files should be in CSV format, including headers as defined in template.csv,
and will be parsed by the csvtojson Node.js library.

## To Dos
* Track tests of features that require two implementations to
interact.  Probably need to add a column that tracks the other
implementation's name (as given by the root name of these files).
* Sort categories together in the output and put the category in a section header instead of a column.
* Internal hyperlinks in assertions should be removed or fixed (rebased to they point to the spec).
* Section references should be resolved or removed.

